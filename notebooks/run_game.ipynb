{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run a Risk Game\n",
        "\n",
        "This needs work but shows how to bring in our game code, and run it in simple ways."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nb_setup import risk_ai_game\n",
        "\n",
        "agents = [\n",
        "    risk_ai_game.AggressiveAgent(0, name=\"Aggressive-P0\"),\n",
        "    risk_ai_game.RandomAgent(1, aggression=0.3, name=\"Random-P1\"),\n",
        "]\n",
        "# I'm running this verbose for now, but to do 1000 runs and collect results, plainly it can't be.\n",
        "risk_ai_game.run_game(agents, max_turns=1000, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TODO:\n",
        "\n",
        "Plainly this needs to be better, so I'll list some obvious improvements below.\n",
        "\n",
        "* A game optoins structure should contain things like\n",
        "    * max_turns\n",
        "    * verbosity and other logging options\n",
        "    * agent setup; what seat is occupied by agent implementation and maybe providing named tags.\n",
        "    * a way to inject a known initial board setup.\n",
        "* Maybe as an additional option we should have a way to inject an object to \"collect\" game data. These objects would vary by notebook, and would be provided game state at various points in our game loop. thus you could (in memory) collect any stats you like inferred off the game state. Something like this (however it's implemented) is pivotal.\n",
        "* Some way to get performance metrics out. Mimicing the metrics used in the course code is probably best."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}